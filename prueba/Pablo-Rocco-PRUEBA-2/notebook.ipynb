{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='logo.png' style='display: block;height: 61px;float: left;padding: .75rem 1.25rem;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 2: Analizado los crímenes en la Ciudad de Nueva York"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado:\n",
    "\n",
    "En esta ocasión trabajaremos con datos públicos del departamento de policía de New York. El dataset es  llamado `stop_and_frisk_data`  y  contiene información sobre interrogaciones y detenciones realizadas por el departamento de policia de NY en la vía pública. El diccionario de atributos se encuentra en el archivo [2009 SQF File Spec.xlsx](./2009_SQF_File_Spec.xlsx).\n",
    "\n",
    "Para todo nuestro estudio utilizaremos los datos correspondientes al año 2009 como conjunto de entrenamiento y los datos del 2010 como conjunto de pruebas. Hay que hacer notar que los datos que estamos utilizando son un muestreo del 1% de la cantidad de registros reales que contiene el dataset, esta decisión fue tomada debido a los largos tiempos de entrenamiento y procesamiento que requiere el volumén de datos reales.\n",
    "\n",
    "* Crea una carpeta de trabajo y guarda todos los archivos correspondientes (notebook, archivosauxiliares y csv).\n",
    "\n",
    "* Una vez terminada la prueba, comprime la carpeta y sube el `.zip` a la seccióncorrespondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivos\n",
    "\n",
    "Para alcanzar el objetivo general, su trabajo se puede desagregar en los siguientes puntos:\n",
    "\n",
    "1. Dado la gran cantidad de atributos, se le entrega un script `preproc_nyc_sqf.py` que normaliza la cantidad de atributos. Haga uso de la función `create_suitable_dataframe` para igualar el benchmark de los atributos.\n",
    "\n",
    "\n",
    "2. Debe analizar de forma exploratoria los atributos. Reporte la cantidad de datos perdidos y presente su esquema de recodificación.\n",
    "\n",
    "\n",
    "3. Generar un modelo predictivo que __condicional__ a las características medidas del sospechoso, prediga si un determinado procedimiento concluirá en un arresto o no. Para ello, guíase por los siguentes lineamientos:\n",
    "  * Entrene por lo menos 1 modelo que sea capaz de predecir si se producirá un arresto o no. Una vez que encuentre un modelo satisfactorio, reporte al menos dos métricas de desempeño.\n",
    "  * Refine aquellos atributos relevantes. Encuentre por lo menos 30 atributos que explique la importancia relativa y ordénelos por orden de importancia.\n",
    "  * Finalmente, reporte la probabilidad que un individuo sea arrestado en uno de los cinco barrios, condicional al género y condicional a la raza.\n",
    "\n",
    "\n",
    "4. Genere al menos cinco modelos predictivos que permitan determinar si el procedimiento policial concluirá en alguna acción violenta.\n",
    "  * Para ello, debe generar un nuevo atributo como vector objetivo que indique cuándo hubo violencia o no. Éste debe ser creado a partir de atributos existentes que indiquen el tipo de violencia. El detalle de los atributos que se consideran violentos se detalla a continuación:\n",
    "```python\n",
    "'pf_hands', 'pf_wall', 'pf_grnd', 'pf_drwep', 'pf_baton', \n",
    "'pf_hcuff', 'pf_pepsp', 'pf_other'\n",
    "```\n",
    "\n",
    "\n",
    "5. Seleccione los 2 mejores modelos, serialicelos y envíelos a evaluación. Recuerde que el modelo serializado debe ser posterior al `fit` , para poder ejecutar `predict` en los nuevos datos.\n",
    "\n",
    "\n",
    "6. La evaluación del modelo será realizada en función a un conjunto de datos reservados al cual no tienen acceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Para resolver el problema planteado, \"determinar si un procedimiento concluirá en un arresto o no\", vamos a usar técnicas y algoritmos de clasificación. Tenemos todos los elementos para resolver un problema de aprendizaje supervisado, ya que tenemos la variable dependiente __arstmade__ (se realizó la una detención) y se plantea como un atributo discreto, en este caso binario. Además disponemos de una base de datos con suficientes muestras. Para entrenar el modelo usaremos los datos del año 2009.\n",
    ">\n",
    "> Inicialmente nos vemos tentados a usar una solución desde la econometría, no obstante las herramientas disponibles para machine learning nos permitiran navegar sobre soluciones y encontrar un modelo de la solución balanceado entre el sesgo y la varianza. Dada la cantidad de atributos nominales y la bondad evaluada en los metodos de __ensambles__ vamos a apostar por obtener buenos resultados con __Random Forest__, __Adaptive Boosting__ y __Gradient Boosting__, no obstante y para efectos de comparacón de modelos, vamos a implementar el algoritmo __SVC__ con Kernel RBF y __LDA__ (análisis lineal discriminante) o __QDA__ (análisis cuadrático discriminante).\n",
    ">\n",
    "> Para __validar la solución__ extraeremos medidas desde la matriz de confusión, revisaremos __accuracy__, __Precision__ y __Recall__, luego revisaremos la media armónica entre Precision y Recall (__F1__). Para terminar usaremos el despliegue de la curva __ROC__ para justificar la bondad del modelo. Es decir, explicar que % adicional explica el modelo por sobre una asignación aleatorea (0.5). Para validar el modelo usaremos datos del __2010__, así revisamos la capacidad de predicción empírica del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector objetivo:\n",
    ">\n",
    "> Para el objetivo 3 nuestro vector objetivo arstmade (se realizó o no la detención).\n",
    ">\n",
    "> Para el objetivo 4 nuestro vector objetivo es una nueva variable que determinará si el procedimiento policial cloncluirá en una acción violenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo de problema:\n",
    ">\n",
    "> Estamos frente a un problema de clasifición con información previa sobre las clases y atributos de un procedimiento policial. En este sentido, tenemos la data para abordar la solución con herramientas de clasificación supervisada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos:\n",
    "\n",
    "> Los hiperparámetros de los modelos serán calibrados mediante búsqueda de grilla con validaciones cruzadas (GridSeachCV), en la medida que se requiera. \n",
    ">\n",
    "> __Gradient Boosting__: \n",
    ">  + param_grid={'learning_rate': [0.01, 0.1, 0.5], 'n_estimators': [50, 100, 500, 1000, 2000]}\n",
    ">  + 5 Validaciones Cruzadas\n",
    ">\n",
    "> __Adaptive Boosting__: \n",
    ">  + param_grid={'learning_rate': [0.01, 0.1, 0.5],'n_estimators': [50, 100, 500, 1000, 2000], 'subsample': [0.1,0.5,0.9]}\n",
    ">  + 5 Validaciones Cruzadas\n",
    ">\n",
    "> __Random Forest__: \n",
    ">  + n_estimators = range(20, 1000, 50)\n",
    ">  + max_features=\"log2\"\n",
    ">  + oob_score = True\n",
    ">\n",
    "> __SVC__ con Kernel __RBF__:\n",
    ">  + param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000], 'gamma': [0.0000001, 0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    ">  + 5 Validaciones Cruzadas\n",
    ">\n",
    "> __QDA__ (análisis cuadrático discriminante): Es probable que dejemos iniciado el hiperparámetro priors, con la probabilidad a priori del 2009.\n",
    ">\n",
    "> __BernoulliNB__ (para objetivo 3) o __MultinomialNB__ (para objetivo 4): Sin modificar hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas:\n",
    "> * Accuracy\n",
    "> * Precision\n",
    "> * Recall\n",
    "> * fi_score\n",
    "> * ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento\n",
    ">\n",
    "> Para el caso de los atributos categoricos se binarizarán y en caso de que los atributos continuos estén desbalanceados procederemos a recodificarlos con su logaritmo. Adicionalmente usaremos la función `create_suitable_dataframe` para igualar el benchmark de los atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspectos computacionales\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se utilizan librerías bases para el tratamiento de datos y algunos aspectos básicos de cálculo y gráficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa libreria para el manejo de bases de datos\n",
    "import pandas as pd\n",
    "# Se importa libreria para el manejo de operaciones de cálculo\n",
    "import numpy as np\n",
    "# Se importa libreria para el manejo de gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "# Se importa libreria para manejo de funciones estadisticas y probabilidades\n",
    "import seaborn as sns\n",
    "# Librería para visualizar patrones de datos perdidos\n",
    "import missingno as msngo\n",
    "# Se importa libreria para el menejo de warning\n",
    "import warnings\n",
    "# Se importa libreria para el trabajo de expresiones reguilares\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Definimos algunos aspectos de ambiente y valores por defecto de visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por defecto, matplotlib crea una figura en una ventana separada.\n",
    "# podemos hacer que las figuras aparezcan en línea dentro del notebook; lo hacemos ejecutando:\n",
    "%matplotlib inline\n",
    "# Se ignoran los warning para evitar suciedad en la ejecución\n",
    "warnings.filterwarnings(action='ignore')\n",
    "# Se define el estilo de gráficos a usar\n",
    "plt.style.use('seaborn-pastel')\n",
    "# Se define el tamaño de los paños de los gráficos por defecto\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "# Dado que vamos a supervisar datos no limitaremos la cantidad de columnas a mostrar en el despliegue del dataframe\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se utilizan librerías de __sklear__ para resolver el problema desde el enfoque de machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método de ensamble secuencial, corrige mediante la función de pérdida usando el descenso por el gradiente.\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Método de ensamble secuencial, corrige por las tasas de clasificaciones incorrectas\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Método de ensamble paralelo, aplica bootstrapping por fila y columnas. \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Método de clasificación mediante la maximización de margen de clases.\n",
    "from sklearn.svm import SVC\n",
    "# Método de Análisis cuadrático discriminante, los atributos tienen una forma cuadrátio.\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# Método que implementa Bayes ingenuo para el caso dos clases y multiclase.\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "# Métricas para evaluar modelos de clasificación\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "# Métodos de selección de muestras\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# Metodo para el procesamiento de los datos en una tuberias\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se utiliza la librería __pickle__ que contiene la función dump, que permite guardar el modelo desarrollado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para hacer dump del modelo y objetos\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se utiliza <strong>librería propia</strong> que continen funciones auxiliares, necesarias para el desarrollo de desafíos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa libreria con funciones auxiliares\n",
    "import ancilliary_funcs as afx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se utiliza <strong>librería helpers.py</strong> que continen funciones auxiliares, necesarias para el desarrollo de desafíos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers as hlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se utiliza __librería preproc_nyc_sqf.py__ provista para abordar temas de procesamiento de atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preproc_nyc_sqf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
