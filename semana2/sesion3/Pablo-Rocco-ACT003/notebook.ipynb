{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='logo.png' style='display: block;height: 61px;float: left;padding: .75rem 1.25rem;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío - Naive Bayes\n",
    "* Para realizar este desafío debes haber revisado la lectura y videos correspondiente a la unidad.\n",
    "* Crea una carpeta de trabajo y guarda todos los archivos correspondientes (notebook y csv).\n",
    "* Una vez terminado el desafío, comprime la carpeta y sube el `.zip` a la seccióncorrespondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "\n",
    "* En esta sesión trabajaremos con una serie de base de datos sobre letras musicales de distintos artistas. Cada uno de los `csv` se encuentra en la carpeta `dump` dentro del proyecto.\n",
    "* Cada csv tiene el nombre del artista a analizar. Los archivos contienen el nombre del artista, el género musical del artista, el nombre de la canción y las letras.\n",
    "* En base a esta información, se les pide un modelo generativo que pueda predecir el género de una canción a patir de la letra de una canción.\n",
    "* Existen 4 géneros que se registran en la base de datos, por lo que para esta actividad trabajaremos con un Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 1: Preparar el ambiente de trabajo\n",
    "\n",
    "___\n",
    "* Importe los módulos `numpy` , `pandas` , `matplotlib` , `seaborn` , `glob` y `os` siguiendo las buenas prácticas. Los últimos dos módulos permitirán realizar la importación de múltiples archivos dentro de la carpeta `dump`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #004085; background-color: #cce5ff;border-color: #b8daff;border-radius: .25rem;padding: .75rem 1.25rem;'>\n",
    "Se utilizan librerías bases para el tratamiento de datos y algunos aspectos básicos de cálculo y gráficos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa libreria para el manejo de bases de datos\n",
    "import pandas as pd\n",
    "# Se importa libreria para el manejo de operaciones de cálculo\n",
    "import numpy as np\n",
    "# Se importa libreria para el manejo de gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "# Se importa libreria para manejo de funciones estadisticas y probabilidades\n",
    "import seaborn as sns\n",
    "# Librería para visualizar patrones de datos perdidos\n",
    "import missingno as msngo\n",
    "# Se importa libreria para el menejo de warning\n",
    "import warnings\n",
    "# Se importa libreria para el menejo de expresiones regulares\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #004085; background-color: #cce5ff;border-color: #b8daff;border-radius: .25rem;padding: .75rem 1.25rem;'>\n",
    "Definimos algunos aspectos de ambiente y valores por defecto.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por defecto, matplotlib crea una figura en una ventana separada.\n",
    "# podemos hacer que las figuras aparezcan en línea dentro del notebook; lo hacemos ejecutando:\n",
    "%matplotlib inline\n",
    "# Se ignoran los warning para evitar suciedad en la ejecución\n",
    "warnings.filterwarnings(action='ignore')\n",
    "# Se define el estilo de gráficos a usar\n",
    "plt.style.use('seaborn')\n",
    "# Se define el tamaño de los paños de los gráficos por defecto\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "# Dado que vamos a supervisar datos no limitaremos la cantidad de columnas a mostrar en el despliegue del dataframe\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #004085; background-color: #cce5ff;border-color: #b8daff;border-radius: .25rem;padding: .75rem 1.25rem;'>\n",
    "Se utiliza <strong>librería propia</strong> que continen funciones auxiliares, necesarias para el desarrollo de desafíos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa libreria con funciones auxiliares\n",
    "import ancilliary_funcs as afx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #004085; background-color: #cce5ff;border-color: #b8daff;border-radius: .25rem;padding: .75rem 1.25rem;'>\n",
    "Se utilizan librerías <strong>sklearn</strong> para el tratamiento de escala de valores de los atributos y seleccion de set de datos:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para la selección de datos entre entrenamiento y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para ello genere un objeto que guarde en una lista todos los archivos alojados en `dump` utilizando `glob.glob` y `os.getcwd` para extraer las rutas absolutas. Posteriormente genere un objeto `pd.DataFrame` que contenga todos los csv.\n",
    "* Asegúrese de eliminar la columna `Unnamed: 0` que se genera por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.getcwd() + '/dump/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_csv = []\n",
    "\n",
    "for filename in file_list:\n",
    "    append_csv.append(pd.read_csv(filename, index_col=None, header=0)\n",
    "                      .drop(columns = 'Unnamed: 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(append_csv)\n",
    "df.columns = ['Artis', 'Genre', 'Song', 'Lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artis</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>rock</td>\n",
       "      <td>Jersey Girl</td>\n",
       "      <td>I got no time for the corner boys \\n Down in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pink Floyd</td>\n",
       "      <td>rock</td>\n",
       "      <td>It Would Be So Nice</td>\n",
       "      <td>It would be so nice \\n It would be so nice \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Mayhem</td>\n",
       "      <td>metal</td>\n",
       "      <td>Illuminate Eliminate</td>\n",
       "      <td>Where I came from... I must return \\n No more,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Weezer</td>\n",
       "      <td>rock</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Unfortunately, we are not licensed to display ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>rock</td>\n",
       "      <td>Darkness On The Edge Of Town</td>\n",
       "      <td>They're still racing out at the Trestles \\n Bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Artis  Genre                          Song  \\\n",
       "83  Bruce Springsteen   rock                   Jersey Girl   \n",
       "15         Pink Floyd   rock           It Would Be So Nice   \n",
       "42             Mayhem  metal          Illuminate Eliminate   \n",
       "89             Weezer   rock                      Everyone   \n",
       "33  Bruce Springsteen   rock  Darkness On The Edge Of Town   \n",
       "\n",
       "                                               Lyrics  \n",
       "83  I got no time for the corner boys \\n Down in t...  \n",
       "15  It would be so nice \\n It would be so nice \\n ...  \n",
       "42  Where I came from... I must return \\n No more,...  \n",
       "89  Unfortunately, we are not licensed to display ...  \n",
       "33  They're still racing out at the Trestles \\n Bu...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9489, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisamos las dimensiones de la base de datos\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #004085; background-color: #cce5ff;border-color: #b8daff;border-radius: .25rem;padding: .75rem 1.25rem;'>\n",
    "La base de datos se compone de 9489 observaciones con 4 atributos. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Descripción de los datos\n",
    "___\n",
    "* Utilizando el objeto creado en el Ejercicio 1, genere dos gráficos de barras que resuman la siguiente información:\n",
    "    - La cantidad de canciones registradas por cada artista, ordenado de mayor a menor.\n",
    "    - La cantidad de canciones registradas en cada género, ordenados de mayor a menor.\n",
    "* Comente sobre las principales tendencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Matriz de ocurrencias\n",
    "\n",
    "\n",
    "__Digresión: Tokenización de Textos__\n",
    "\n",
    "Para poder trabajar con textos, debemos pasarlos a una __matriz dispersa__, donde cada fila representará una entrada (en este caso, una canción), y cada columna __representará una palabra (token)__. Este es el proceso de tokenización: Identificar la ocurrencia de una palabra específica dentro de un conjunto de textos (corpus). El tokenizador más simple\n",
    "`sklearn.feature_extraction.text.CountVectorizer` genera una colección de textos a una matriz que representa la frecuencia __dentro del texto__ de una palabra específica. El tokenizador funciona de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importe la clase `CountVectorizer` dentro de los módulos `feature_extraction.text` de la librería `sklearn`. Lea la documentación asociada a ésta. ¿Cuál es el objetivo de esta clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer_fit = count_vectorizer.fit_transform(df['Lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aplique la clase para extraer las 100 palabras más repetidas en toda la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24950</th>\n",
       "      <td>like</td>\n",
       "      <td>19629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12859</th>\n",
       "      <td>don</td>\n",
       "      <td>17398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23856</th>\n",
       "      <td>know</td>\n",
       "      <td>14962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18439</th>\n",
       "      <td>got</td>\n",
       "      <td>14171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23287</th>\n",
       "      <td>just</td>\n",
       "      <td>13978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25479</th>\n",
       "      <td>love</td>\n",
       "      <td>11268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48591</th>\n",
       "      <td>yeah</td>\n",
       "      <td>11071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25165</th>\n",
       "      <td>ll</td>\n",
       "      <td>10028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29851</th>\n",
       "      <td>oh</td>\n",
       "      <td>9879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>cause</td>\n",
       "      <td>8356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43795</th>\n",
       "      <td>time</td>\n",
       "      <td>8037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24735</th>\n",
       "      <td>let</td>\n",
       "      <td>8009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8724</th>\n",
       "      <td>come</td>\n",
       "      <td>7866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26067</th>\n",
       "      <td>man</td>\n",
       "      <td>7467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>ain</td>\n",
       "      <td>7323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25969</th>\n",
       "      <td>make</td>\n",
       "      <td>6821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>baby</td>\n",
       "      <td>6735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47086</th>\n",
       "      <td>want</td>\n",
       "      <td>6698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37216</th>\n",
       "      <td>say</td>\n",
       "      <td>6684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36025</th>\n",
       "      <td>right</td>\n",
       "      <td>6207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38497</th>\n",
       "      <td>shit</td>\n",
       "      <td>6128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46260</th>\n",
       "      <td>ve</td>\n",
       "      <td>5924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47265</th>\n",
       "      <td>way</td>\n",
       "      <td>5713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24875</th>\n",
       "      <td>life</td>\n",
       "      <td>5710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47080</th>\n",
       "      <td>wanna</td>\n",
       "      <td>5356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18334</th>\n",
       "      <td>gonna</td>\n",
       "      <td>5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48695</th>\n",
       "      <td>yo</td>\n",
       "      <td>4859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15760</th>\n",
       "      <td>feel</td>\n",
       "      <td>4704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28852</th>\n",
       "      <td>need</td>\n",
       "      <td>4579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17232</th>\n",
       "      <td>fuck</td>\n",
       "      <td>4537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>big</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12035</th>\n",
       "      <td>did</td>\n",
       "      <td>2359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20353</th>\n",
       "      <td>hold</td>\n",
       "      <td>2326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12054</th>\n",
       "      <td>die</td>\n",
       "      <td>2303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>bad</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24080</th>\n",
       "      <td>la</td>\n",
       "      <td>2202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43449</th>\n",
       "      <td>things</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36670</th>\n",
       "      <td>run</td>\n",
       "      <td>2176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44828</th>\n",
       "      <td>try</td>\n",
       "      <td>2165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36254</th>\n",
       "      <td>rock</td>\n",
       "      <td>2165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45178</th>\n",
       "      <td>uh</td>\n",
       "      <td>2146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24911</th>\n",
       "      <td>light</td>\n",
       "      <td>2101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44953</th>\n",
       "      <td>turn</td>\n",
       "      <td>2078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10993</th>\n",
       "      <td>dead</td>\n",
       "      <td>2076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24571</th>\n",
       "      <td>leave</td>\n",
       "      <td>2056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20258</th>\n",
       "      <td>hit</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19457</th>\n",
       "      <td>hard</td>\n",
       "      <td>2037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18326</th>\n",
       "      <td>gone</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44032</th>\n",
       "      <td>tonight</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43768</th>\n",
       "      <td>till</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20131</th>\n",
       "      <td>high</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5354</th>\n",
       "      <td>boy</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40164</th>\n",
       "      <td>soul</td>\n",
       "      <td>1921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22016</th>\n",
       "      <td>inside</td>\n",
       "      <td>1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14892</th>\n",
       "      <td>everybody</td>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11041</th>\n",
       "      <td>death</td>\n",
       "      <td>1897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24615</th>\n",
       "      <td>left</td>\n",
       "      <td>1895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43442</th>\n",
       "      <td>thing</td>\n",
       "      <td>1882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14305</th>\n",
       "      <td>end</td>\n",
       "      <td>1829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41068</th>\n",
       "      <td>stay</td>\n",
       "      <td>1828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word   freq\n",
       "24950       like  19629\n",
       "12859        don  17398\n",
       "23856       know  14962\n",
       "18439        got  14171\n",
       "23287       just  13978\n",
       "25479       love  11268\n",
       "48591       yeah  11071\n",
       "25165         ll  10028\n",
       "29851         oh   9879\n",
       "7009       cause   8356\n",
       "43795       time   8037\n",
       "24735        let   8009\n",
       "8724        come   7866\n",
       "26067        man   7467\n",
       "1265         ain   7323\n",
       "25969       make   6821\n",
       "3022        baby   6735\n",
       "47086       want   6698\n",
       "37216        say   6684\n",
       "36025      right   6207\n",
       "38497       shit   6128\n",
       "46260         ve   5924\n",
       "47265        way   5713\n",
       "24875       life   5710\n",
       "47080      wanna   5356\n",
       "18334      gonna   5121\n",
       "48695         yo   4859\n",
       "15760       feel   4704\n",
       "28852       need   4579\n",
       "17232       fuck   4537\n",
       "...          ...    ...\n",
       "4251         big   2363\n",
       "12035        did   2359\n",
       "20353       hold   2326\n",
       "12054        die   2303\n",
       "3128         bad   2217\n",
       "24080         la   2202\n",
       "43449     things   2186\n",
       "36670        run   2176\n",
       "44828        try   2165\n",
       "36254       rock   2165\n",
       "45178         uh   2146\n",
       "24911      light   2101\n",
       "44953       turn   2078\n",
       "10993       dead   2076\n",
       "24571      leave   2056\n",
       "20258        hit   2048\n",
       "19457       hard   2037\n",
       "18326       gone   2016\n",
       "44032    tonight   2010\n",
       "43768       till   2003\n",
       "20131       high   1989\n",
       "5354         boy   1971\n",
       "40164       soul   1921\n",
       "22016     inside   1915\n",
       "14892  everybody   1903\n",
       "11041      death   1897\n",
       "24615       left   1895\n",
       "43442      thing   1882\n",
       "14305        end   1829\n",
       "41068       stay   1828\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = count_vectorizer.get_feature_names()\n",
    "words_freq = count_vectorizer_fit.toarray().sum(axis=0)\n",
    "df_words = pd.DataFrame({'word': words, 'freq': words_freq})\n",
    "df_words_100 = df_words.sort_values(by='freq', ascending=False).head(100)\n",
    "df_words_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Genere una función que replique el procedimiento para cada uno de los géneros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ocurrencia(df, column, value, top = 100):\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    count_vectorizer_fit = count_vectorizer.fit_transform(df[df[column]==value]['Lyrics'])\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    words_freq = count_vectorizer_fit.toarray().sum(axis=0)\n",
    "    df_words = pd.DataFrame({'word': words, 'freq': words_freq})\n",
    "    df_words_100 = df_words.sort_values(by='freq', ascending=False).head(100)\n",
    "    return df_words_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word   freq\n",
      "18361    like  12055\n",
      "13533     got   7613\n",
      "9466      don   7604\n",
      "17532    know   6535\n",
      "17082    just   6272\n",
      "28298    shit   5546\n",
      "909       ain   4782\n",
      "5237    cause   4737\n",
      "19198     man   4481\n",
      "35793      yo   4425\n",
      "35721    yeah   4087\n",
      "21434   nigga   3993\n",
      "12661    fuck   3705\n",
      "18202     let   3436\n",
      "18531      ll   3391\n",
      "19137    make   3099\n",
      "10400      em   3078\n",
      "6578     come   2992\n",
      "32299    time   2982\n",
      "26442   right   2936\n",
      "27334     say   2836\n",
      "18783    love   2830\n",
      "35636      ya   2765\n",
      "21439  niggas   2666\n",
      "21975      oh   2375\n",
      "34582    want   2360\n",
      "34576   wanna   2230\n",
      "18308    life   2143\n",
      "3197    bitch   2123\n",
      "1821      ass   2054\n",
      "...       ...    ...\n",
      "33080     try   1013\n",
      "14824    high    987\n",
      "26955     run    986\n",
      "14787     hey    985\n",
      "35421    word    978\n",
      "14313    hard    971\n",
      "11238    face    970\n",
      "14531    hear    967\n",
      "29513     son    959\n",
      "2255      bad    939\n",
      "23749    play    935\n",
      "21454   night    924\n",
      "18074   leave    919\n",
      "25459  really    916\n",
      "18649    long    915\n",
      "8039     damn    910\n",
      "5586    check    900\n",
      "30299    stay    864\n",
      "3947      boy    857\n",
      "14999    hold    857\n",
      "2726     beat    850\n",
      "15047    home    848\n",
      "32053   thing    830\n",
      "10453  eminem    829\n",
      "15363     huh    827\n",
      "31521    talk    823\n",
      "18961      ma    821\n",
      "34088   verse    820\n",
      "5866   chorus    811\n",
      "3911     bout    808\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "           word  freq\n",
      "7883       life  1233\n",
      "4088        don  1181\n",
      "3284      death  1139\n",
      "13844      time  1096\n",
      "7977         ll  1069\n",
      "7622       know  1048\n",
      "7524       just   993\n",
      "1458      blood   884\n",
      "7914       like   869\n",
      "3766        die   866\n",
      "14681        ve   847\n",
      "5911        god   796\n",
      "929        away   776\n",
      "3269       dead   771\n",
      "4972       eyes   770\n",
      "15031       way   724\n",
      "15287     world   700\n",
      "2482       come   631\n",
      "6411       hell   598\n",
      "15384      yeah   583\n",
      "4525        end   581\n",
      "9525       pain   567\n",
      "8562       mind   543\n",
      "7575       kill   543\n",
      "14951      want   531\n",
      "8230        man   531\n",
      "5961        got   530\n",
      "5137       feel   516\n",
      "9281         oh   509\n",
      "7178     inside   494\n",
      "...         ...   ...\n",
      "9954      place   261\n",
      "3232       dark   250\n",
      "1211    believe   247\n",
      "5451    forever   244\n",
      "3243   darkness   241\n",
      "6248      hands   240\n",
      "14156     truth   238\n",
      "11639     satan   236\n",
      "4312      earth   234\n",
      "7971      lives   233\n",
      "5627    fucking   231\n",
      "7879        lie   230\n",
      "13285       sun   228\n",
      "13720    things   228\n",
      "3378       deep   226\n",
      "9331       open   226\n",
      "7580    killing   226\n",
      "4163      dream   223\n",
      "378         ain   222\n",
      "6242       hand   222\n",
      "2484      comes   221\n",
      "6536       home   221\n",
      "6636      human   220\n",
      "1767       burn   219\n",
      "7975     living   218\n",
      "13839      till   215\n",
      "5939       good   214\n",
      "11666      save   213\n",
      "13041      stop   211\n",
      "8034       lord   210\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "            word  freq\n",
      "3135         don  3329\n",
      "6219        like  2995\n",
      "6390        love  2914\n",
      "7455          oh  2737\n",
      "5918        know  2677\n",
      "5748        just  2610\n",
      "12163       yeah  2244\n",
      "655         baby  2192\n",
      "4582         got  2156\n",
      "6290          ll  1743\n",
      "6150         let  1672\n",
      "11756      wanna  1652\n",
      "11759       want  1529\n",
      "1690       cause  1523\n",
      "6503        make  1409\n",
      "2149        come  1383\n",
      "9236         say  1259\n",
      "10988       time  1225\n",
      "11566         ve  1122\n",
      "265          ain  1065\n",
      "4456        girl  1020\n",
      "3854        feel  1016\n",
      "10774       tell  1012\n",
      "7230        need  1000\n",
      "11816        way   934\n",
      "8929       right   886\n",
      "4549       gonna   842\n",
      "8673      really   842\n",
      "12139         ya   801\n",
      "6191        life   718\n",
      "...          ...   ...\n",
      "8665        real   341\n",
      "9536        shit   337\n",
      "4546        gone   331\n",
      "12191         yo   329\n",
      "7845      people   323\n",
      "10875     things   323\n",
      "6097       leave   319\n",
      "4922        hear   312\n",
      "3723        face   308\n",
      "8042        play   305\n",
      "10206       stay   305\n",
      "10896    thought   304\n",
      "1297       break   303\n",
      "2623          da   302\n",
      "5170         hot   300\n",
      "10981       till   298\n",
      "7299       nigga   298\n",
      "6338        long   298\n",
      "1624        care   297\n",
      "11282        try   288\n",
      "4120     forever   287\n",
      "964          big   286\n",
      "3760        fall   286\n",
      "7271         new   285\n",
      "2452       crazy   282\n",
      "878      believe   282\n",
      "10873      thing   274\n",
      "3641   everybody   270\n",
      "3429          eh   269\n",
      "1865       chick   267\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "         word  freq\n",
      "5282      don  5284\n",
      "10362    love  5114\n",
      "9697     know  4702\n",
      "12077      oh  4258\n",
      "20056    yeah  4157\n",
      "9517     just  4103\n",
      "7606      got  3872\n",
      "10220      ll  3825\n",
      "10113    like  3710\n",
      "19093      ve  2877\n",
      "3568     come  2860\n",
      "18064    time  2734\n",
      "1188     baby  2711\n",
      "7573    gonna  2497\n",
      "10012     let  2410\n",
      "19406    want  2278\n",
      "15141     say  2174\n",
      "19506     way  2151\n",
      "10587     man  2004\n",
      "1148     away  1991\n",
      "14601   right  1980\n",
      "10553    make  1893\n",
      "11808   night  1856\n",
      "4518      day  1784\n",
      "2837    cause  1750\n",
      "8260      hey  1746\n",
      "6491     feel  1620\n",
      "10072    life  1616\n",
      "10204  little  1609\n",
      "11705    need  1494\n",
      "...       ...   ...\n",
      "9037   inside   676\n",
      "18522     try   676\n",
      "10205    live   664\n",
      "7997     hard   655\n",
      "9951    leave   655\n",
      "16440    soul   635\n",
      "11768     new   631\n",
      "14897     run   625\n",
      "7556    going   619\n",
      "5880      end   612\n",
      "5418    dream   608\n",
      "1677      big   603\n",
      "17897   thing   593\n",
      "7537      god   590\n",
      "20003   wrong   588\n",
      "8279     high   587\n",
      "4961      die   585\n",
      "13018   place   584\n",
      "16045     sky   584\n",
      "5819       em   583\n",
      "16981    stop   581\n",
      "14747    roll   573\n",
      "4430    dance   567\n",
      "9965     left   565\n",
      "16772   stand   563\n",
      "18247    town   558\n",
      "6337     fall   548\n",
      "14819   round   548\n",
      "4527     days   542\n",
      "10335    lost   541\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "for gen in df['Genre'].unique():\n",
    "    print(Ocurrencia(df, 'Genre', gen, top = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #854000; background-color: #ffe5e5;border-color: #ffdaff;border-radius: .25rem;padding: .75rem 1.25rem;'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
